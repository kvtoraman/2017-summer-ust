{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of {Doc2Vec Tutorial on the Lee Dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\py27\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?\n",
    "\n",
    "Doc2Vec is an NLP tool for representing documents as a vector and is a generalizing of the Word2Vec method. This tutorial will serve as an introduction to Doc2Vec and present ways to train and assess a Doc2Vec model.\n",
    "## Resources\n",
    "\n",
    "* [Word2Vec Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [Doc2Vec Paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "* [Dr. Michael D. Lee's Website](http://faculty.sites.uci.edu/mdlee)\n",
    "* [Lee Corpus](http://faculty.sites.uci.edu/mdlee/similarity-data/)\n",
    "* [IMDB Doc2Vec Tutorial](doc2vec-IMDB.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get going, we'll need to have a set of documents to train our doc2vec model. In theory, a document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book. In NLP parlance a collection or set of documents is often referred to as a <b>corpus</b>. \n",
    "\n",
    "For this tutorial, we'll be training our model using the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included in gensim. This corpus contains 314 documents selected from the Australian Broadcasting\n",
    "Corporation’s news mail service, which provides text e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter [Lee Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) which contains 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "#test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = 'rev_kkma_data_all_1col_train.dat'\n",
    "lee_test_file = 'rev_kkma_data_all_1col_test.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a function to open the train/test file (with latin encoding), read the file line-by-line, pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Note that, for a given file (aka corpus), each continuous line constitutes a single document and the length of each line (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the zero-based line number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_elems = line.split('\\t')\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line_elems[1])\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line_elems[1]), [line_elems[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print lee_train_file\n",
    "#print lee_test_file\n",
    "#print (train_corpus[:1])\n",
    "#for x in train_corpus[0].words:\n",
    " #   print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(test_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Doc2Vec Object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 words and iterating over the training corpus 55 times. We set the minimum word count to 2 in order to give higher frequency words more weighting. Model accuracy can be improved by increasing the number of iterations but this generally increases the training time. Small datasets with short documents, like this one, can benefit from more training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=15, min_count=2, iter=20)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial with: 10 100\n",
      "Train:0.47972972973\n",
      "Test:0.383177570093\n",
      "Trial with: 10 105\n",
      "Train:0.480249480249\n",
      "Test:0.378504672897\n",
      "Trial with: 10 110\n",
      "Train:0.491683991684\n",
      "Test:0.378504672897\n",
      "Trial with: 10 115\n",
      "Train:0.511434511435\n",
      "Test:0.364485981308\n",
      "Trial with: 10 120\n",
      "Train:0.514033264033\n",
      "Test:0.378504672897\n",
      "Trial with: 10 125\n",
      "Train:0.524948024948\n",
      "Test:0.420560747664\n",
      "Trial with: 10 130\n",
      "Train:0.518711018711\n",
      "Test:0.401869158879\n",
      "Trial with: 10 135\n",
      "Train:0.531704781705\n",
      "Test:0.392523364486\n",
      "Trial with: 10 140\n",
      "Train:0.543139293139\n",
      "Test:0.42523364486\n",
      "Trial with: 10 145\n",
      "Train:0.528066528067\n",
      "Test:0.38785046729\n",
      "Trial with: 10 150\n",
      "Train:0.553014553015\n",
      "Test:0.434579439252\n",
      "Trial with: 10 155\n",
      "Train:0.538981288981\n",
      "Test:0.415887850467\n",
      "Trial with: 10 160\n",
      "Train:0.558212058212\n",
      "Test:0.420560747664\n",
      "Trial with: 10 165\n",
      "Train:0.550935550936\n",
      "Test:0.397196261682\n",
      "Trial with: 10 170\n",
      "Train:0.540540540541\n",
      "Test:0.406542056075\n",
      "Trial with: 10 175\n",
      "Train:0.547817047817\n",
      "Test:0.448598130841\n",
      "Trial with: 10 180\n",
      "Train:0.570686070686\n",
      "Test:0.42523364486\n",
      "Trial with: 10 185\n",
      "Train:0.563929313929\n",
      "Test:0.415887850467\n",
      "Trial with: 10 190\n",
      "Train:0.566008316008\n",
      "Test:0.420560747664\n",
      "Trial with: 10 195\n",
      "Train:0.58264033264\n",
      "Test:0.46261682243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def trial(the_size,the_iter):\n",
    "    global model\n",
    "    print(\"Trial with:\",the_size,the_iter)\n",
    "    model = gensim.models.doc2vec.Doc2Vec(size=the_size, min_count=2, iter=the_iter)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    print('Train:',end='')\n",
    "    tester(train_corpus)\n",
    "    print('Test:',end='')\n",
    "    tester(test_corpus)\n",
    "for i in range(100,200,5):\n",
    "    trial(the_size=10,the_iter=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Train\n",
    "\n",
    "If the BLAS library is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2 minutes, so use BLAS if you value your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35085663"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note is that you can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the `model.infer_vector` function. This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EI05', 0.806117057800293),\n",
       " ('EE12', 0.8059660196304321),\n",
       " ('ED06', 0.8056091666221619),\n",
       " ('EA09', 0.7554200887680054),\n",
       " ('ED08', 0.735058069229126)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([u'데이터', u'알고리즘', u'소프트웨어'])\n",
    "inferred_vector = model.infer_vector(train_corpus[0].words)\n",
    "model.docvecs.most_similar([inferred_vector], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess our new model, we'll first infer new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity. Basically, we're pretending as if the training corpus is some new unseen data and then seeing how they compare with the trained model. The expectation is that we've likely overfit our model (i.e., all of the ranks will be less than 2) and so we should be able to find similar documents very easily. Additionally, we'll keep track of the second ranks for a comparison of less similar documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tester(corpus):\n",
    "    total_success = 0\n",
    "    total_data = 0\n",
    "    #WARNING : remove divide 4\n",
    "    for doc_id in range(len(corpus)//4):\n",
    "        inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=5)\n",
    "        score = corpus[doc_id].tags[0] in [doc_tag for doc_tag, sim in sims]\n",
    "        if score:\n",
    "            total_success += 1\n",
    "        total_data += 1\n",
    "\n",
    "    print(total_success/total_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.622431747819\n",
      "0.582990115321\n"
     ]
    }
   ],
   "source": [
    "tester(train_corpus)\n",
    "tester(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('trained_with size=10&iter=195')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most similar to itself and about 5% of the time it is mistakenly most similar to another document. the checking of an inferred-vector against a training-vector is a sort of 'sanity check' as to whether the model is behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.9350012540817261): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "MEDIAN (133, 0.2410505712032318): «the hunt for osama bin laden has shifted to the forests around the cave complex of tora bora after swoop through the last caves failed to reveal any sign of the saudi born fugitive us special forces are now combing the forests alongside anti taliban militias up to al qaeda fighters are believed to have scattered into the hills many heading south toward the pakistan border local commanders have warned they will shoot any villager who shelters them us bombing runs have eased off in the past hours as american special forces move to deeper into the forest to coordinate the hunt earlier us forces intercepted voice communication they believed could be bin laden speaking by short range radio to his fighters however senior afghan commander haji zaman said he believed bin laden had left the tora bora area us defence secretary donald rumsfeld on visit to us troops at baghram air base outside kabul said the battle against the taliban and al qaeda was not over there are still pockets of taliban and al qaeda forces that have drifted into the mountains and could reform and there is good deal yet to be done he said»\n",
      "\n",
      "LEAST (261, -0.050494786351919174): «afghan opposition leaders meeting in germany have reached an agreement after seven days of talks on the structure of an interim post taliban government for afghanistan the agreement calls for the immediate assembly of temporary group of multi national peacekeepers in kabul and possibly other areas the four afghan factions have approved plan for member ruling council composed of chairman five deputy chairmen and other members the council would govern afghanistan for six months at which time traditional afghan assembly called loya jirga would be convened to decide on more permanent structure the agreement calls for elections within two years»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document is has a similarity score of ~80% (or higher). However, the similarity score for the second ranked documents should be significantly lower (assuming the documents are in fact different) and the reasoning becomes obvious when we examine the text itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (94): «foreign minister alexander downer says the commonwealth democracy watchdog should put zimbabwe formally on its agenda in the first step to possible suspension from the organisation mr downer says ministers from the commonwealth ministerial action group cmag should review whether the reported violence and intimidation in zimbabwe means it has violated the commonwealth code of good governance cmag ministers from australia bangladesh barbados botswana britain canada malaysia and nigeria will meet in london tomorrow for talks on zimbabwe in recent meetings they have suspended both fiji and pakistan following military coups however their talks on the violent campaign of farm occupations in zimbabwe have been restricted to informal discussions as president robert mugabe government holds power through recognised elections mr downer also says the commonwealth ministers should maintain pressure on president mugabe to allow international observers to oversee presidential elections next march»\n",
      "\n",
      "Similar Document (80, 0.7893248796463013): «zimbabwe has been given five weeks to stop the political violence and invasions of white owned farms or face possible suspension from the commonwealth meeting in london of the commonwealth ministerial action group has listed zimbabwe the first step ahead of what could mean much tougher action under pressure from australia and the united kingdom the issue of zimbabwe consistent breach of democratic principles under the harare declaration is finally and formally on the table the commonwealth group is waiting for response from zimbabwe to request to allow observers for the upcoming election australia foreign minister alexander downer says if there is not substantial change in zimbabwe significant sanctions are possible the commonwealth ministerial action group does have number of weapons available to it and one of them is suspension he said at the same time fiji suspension was lifted after its return to democracy»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_corpus))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same approach above, we'll infer the vector for a randomly chosen test document, and compare the document to our model by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (46): «the river elbe surged to an all time record high friday flooding more districts of the historic city of dresden as authorities scrambled to evacuate tens of thousands of residents in the worst flooding to hit central europe in memory in the czech republic authorities were counting the cost of the massive flooding as people returned to the homes and the vlava river receded revealing the full extent of the damage to lives and landmarks»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (9, 0.5827658772468567): «some roads are closed because of dangerous conditions caused by bushfire smoke motorists are being asked to avoid the hume highway between picton road and the illawarra highway where police have reduced the speed limit from kilometres an hour to in southern sydney picton road is closed between wilton and bulli appin road is closed from appin to bulli tops and all access roads to royal national park are closed motorists are also asked to avoid the illawarra highway between the hume highway and robertson and the great western highway between penrith and springwood because of reduced visibility in north western sydney only local residents are allowed to use wisemans ferry road and upper color road under police escort»\n",
      "\n",
      "MEDIAN (287, 0.06506192684173584): «royal commission will begin this morning in sydney into the collapse of insurance giant hih while the commission held an initial procedural hearing in september today the public hearings will begin more than eight months after the company was placed into provisional liquidation more than one million pages of documents have already been subpoenaed from witnesses including former directors the australian securities and investments commission asic and the prudential regulatory authority the terms of reference include determining what contributed to the collapse whether any laws were broken and whether regulations need to be changed western australian justice neville owen heads the commission but today it is expected to hear mainly from counsel assisting wayne martin qc spokesman for the commission john dickie says it faces great challenge the issues are quite complex really and certainly think it the first one into corporation collapse like this one mr dickie said the inquiry is expected to be finished by the end of next june»\n",
      "\n",
      "LEAST (17, -0.31862398982048035): «spain has begun its hopman cup campaign in perth with victory over argentina arantxa sanchez vicario and tommy robredoboth won their singles matches and then teamed to win the mixed doubles sanchez vicario says she is hoping to win her second hopman cup title after winning the tournament with her brother emilio in it would be very nice to start the year off and as say it always tough but it very good start for me and looking forward with tommy to see if we can be the champions again she said today the united states will play france meanwhile world number one lleyton hewitt says he will not be putting pressure on himself to win next month australian tennis open in melbourne hewitt yesterday teamed with fellow australian alicia molik to beat switzerland in their opening tie at the hopman cup in perth hewitt says his first objective will be to reach the second week of the grand slam event think if play my best tennis and give per cent no matter who play think in with good chance of getting through to the second week and if that happens then most times in grand slam it sort of anyone tournament from there he said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus))\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
