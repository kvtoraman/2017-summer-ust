{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation of {Doc2Vec Tutorial on the Lee Dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\envs\\py27\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?\n",
    "\n",
    "Doc2Vec is an NLP tool for representing documents as a vector and is a generalizing of the Word2Vec method. This tutorial will serve as an introduction to Doc2Vec and present ways to train and assess a Doc2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Word2Vec Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [Doc2Vec Paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "* [Dr. Michael D. Lee's Website](http://faculty.sites.uci.edu/mdlee)\n",
    "* [Lee Corpus](http://faculty.sites.uci.edu/mdlee/similarity-data/)\n",
    "* [IMDB Doc2Vec Tutorial](doc2vec-IMDB.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get going, we'll need to have a set of documents to train our doc2vec model. In theory, a document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book. In NLP parlance a collection or set of documents is often referred to as a <b>corpus</b>. \n",
    "\n",
    "For this tutorial, we'll be training our model using the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included in gensim. This corpus contains 314 documents selected from the Australian Broadcasting\n",
    "Corporation’s news mail service, which provides text e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter [Lee Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) which contains 50 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "#test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = 'rev_kkma_data_all_1col_train.dat'\n",
    "lee_test_file = 'rev_kkma_data_all_1col_test.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a function to open the train/test file (with latin encoding), read the file line-by-line, pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc), and return a list of words. Note that, for a given file (aka corpus), each continuous line constitutes a single document and the length of each line (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the zero-based line number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_elems = line.split('\\t')\n",
    "            if line_elems[0][0:2] != 'EE':\n",
    "                #print(line_elems[0][0:2])\n",
    "                continue\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line_elems[1])\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line_elems[1]), [line_elems[0],line_elems[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=[u'mobile', u'web', u'app', u'process', u'oriented', u'gateway', u'server', u'service', u'platform', u'encryption', u'\\uc2a4\\ub9c8\\ud2b8', u'\\ub2e8\\ub9d0\\uae30', u'\\uae30\\ubc18', u'\\uc5c5\\ubb34', u'\\uc5c5\\ubb34\\uc2dc\\uc2a4\\ud15c', u'\\uc2dc\\uc2a4\\ud15c', u'\\uc5f0\\ub3d9', u'\\ubaa8\\ubc14\\uc77c', u'\\ud50c\\ub9ac\\ucf00\\uc774\\uc158', u'\\ucf00\\uc774', u'\\uc11c\\ube44\\uc2a4', u'\\ucee8\\ubc84\\uc804\\uc2a4', u'\\ud50c\\ub7ab\\ud3fc', u'\\uae30\\uc220', u'\\uac1c\\ubc1c', u'\\ud504\\ub85c\\uc138\\uc2a4', u'\\ud504\\ub85c\\uc138\\uc2a4\\uc911\\uc2ec\\ud615', u'\\uc911\\uc2ec', u'\\uac8c\\uc774\\ud2b8\\uc6e8\\uc774', u'\\uac8c\\uc774\\ud2b8\\uc6e8\\uc774\\uc11c\\ubc84', u'\\uc11c\\ubc84', u'\\uc11c\\ube44\\uc2a4\\ud50c\\ub7ab\\ud3fc', u'\\uc554\\ud638\\ud654', u'\\uae30\\uc5c5', u'\\uc5f0\\uacc4', u'\\ubbf8\\ub4e4', u'\\ubbf8\\ub4e4\\uc6e8\\uc5b4', u'\\uc6e8\\uc5b4', u'\\ud504\\ub808\\uc784', u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c', u'\\uc6cc\\ud06c', u'\\uc801\\uc6a9', u'\\uacf5\\uc778', u'\\uc778\\uc99d', u'\\uc870\\ud569', u'\\uc870\\ud569\\ud615', u'\\ud328\\ud0a4\\uc9c0', u'\\ub370\\uc774\\ud130', u'\\ud655\\ubcf4', u'\\uccb4\\uacc4', u'\\uad6c\\ucd95', u'\\ubd80\\ubb38', u'\\ud2b9\\ud654', u'\\uc2dc\\ubc94', u'\\uacbd\\uc81c\\uc801', u'\\ud6a8\\uacfc', u'\\uc2dc\\ud5d8', u'\\ucc38\\uc5ec', u'\\uc2e0\\uaddc', u'\\uc2dc\\uc7a5', u'\\ube44\\uc988\\ub2c8\\uc2a4', u'\\ubaa8\\ub378', u'\\ubc1c\\uad74', u'\\uad50\\uc721', u'\\ucee8\\uc124\\ud305', u'\\uc9c1\\uc811\\uc801', u'\\ubcf4\\uae09', u'\\ub2e4\\uc591', u'\\ucc44\\ub110', u'\\ud65c\\uc6a9', u'\\ud64d\\ubcf4', u'\\uc0b0\\uc5c5', u'\\uc0b0\\uc5c5\\uc778\\ub825', u'\\uc778\\ub825', u'\\uc721\\uc131', u'\\ucd5c\\uc2e0', u'\\ud655\\uc0b0', u'\\uc8fc\\ub3c4', u'\\uc5c5\\ubb34\\ud601\\uc2e0', u'\\ud601\\uc2e0', u'\\uac15\\ud654', u'\\uae30\\uc220\\uc801', u'\\uae30\\uc874', u'\\uad6c\\ud604', u'\\uae30\\uc5ec', u'\\ud50c\\ub7ab\\ud3fc\\uac04', u'\\ud45c\\uc900', u'\\ud45c\\uc900\\ud654', u'\\uae30\\uc220\\uac15\\ud654', u'\\ubd84\\ub9ac', u'\\uc644\\uc131', u'\\uc644\\uc131\\ub3c4\\uac00', u'\\ub3c4\\uac00', u'\\uc0ac\\ud68c\\uc801', u'\\ubcf4\\uc548', u'\\ubcf4\\uc548\\ubaa8\\ub4c8', u'\\ubaa8\\ub4c8', u'\\uac1c\\uc778', u'\\uac1c\\uc778\\uc815\\ubcf4', u'\\uc815\\ubcf4', u'\\uc9c0\\uc801', u'\\uc790\\uc0b0', u'\\uc720\\ucd9c', u'\\ubc29\\uc9c0', u'\\ube44\\uc988', u'\\ub2c8\\uc2a4', u'\\ub3c4\\uba54\\uc778', u'\\uac00\\uc18d\\ud654'], tags=['EE02', 'EE02'])]\n"
     ]
    }
   ],
   "source": [
    "#print lee_train_file\n",
    "#print lee_test_file\n",
    "print (train_corpus[:1])\n",
    "#for x in train_corpus[0].words:\n",
    " #   print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Doc2Vec Object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 words and iterating over the training corpus 55 times. We set the minimum word count to 2 in order to give higher frequency words more weighting. Model accuracy can be improved by increasing the number of iterations but this generally increases the training time. Small datasets with short documents, like this one, can benefit from more training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=15, min_count=2, iter=20)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial with: 10 5\n",
      "Train:0.652304009575\n",
      "Test:0.541237113402\n",
      "Trial with: 10 10\n",
      "Train:0.694195092759\n",
      "Test:0.618556701031\n",
      "Trial with: 10 15\n",
      "Train:0.731897067624\n",
      "Test:0.649484536082\n",
      "Trial with: 10 20\n",
      "Train:0.748055056852\n",
      "Test:0.639175257732\n",
      "Trial with: 10 25\n",
      "Train:0.749850388989\n",
      "Test:0.701030927835\n",
      "Trial with: 10 30\n",
      "Train:0.742070616397\n",
      "Test:0.685567010309\n",
      "Trial with: 10 35\n",
      "Train:0.755834829443\n",
      "Test:0.716494845361\n",
      "Trial with: 10 40\n",
      "Train:0.760622381807\n",
      "Test:0.685567010309\n",
      "Trial with: 10 45\n",
      "Train:0.742070616397\n",
      "Test:0.675257731959\n",
      "Trial with: 10 50\n",
      "Train:0.767205266308\n",
      "Test:0.680412371134\n",
      "Trial with: 10 55\n",
      "Train:0.76959904249\n",
      "Test:0.701030927835\n",
      "Trial with: 10 60\n",
      "Train:0.77618192699\n",
      "Test:0.716494845361\n",
      "Trial with: 10 65\n",
      "Train:0.754637941352\n",
      "Test:0.726804123711\n",
      "Trial with: 10 70\n",
      "Train:0.766008378217\n",
      "Test:0.731958762887\n",
      "Trial with: 10 75\n",
      "Train:0.771992818671\n",
      "Test:0.737113402062\n",
      "Trial with: 10 80\n",
      "Train:0.77079593058\n",
      "Test:0.762886597938\n",
      "Trial with: 10 85\n",
      "Train:0.777977259126\n",
      "Test:0.721649484536\n",
      "Trial with: 10 90\n",
      "Train:0.757031717534\n",
      "Test:0.690721649485\n",
      "Trial with: 10 95\n",
      "Train:0.759425493716\n",
      "Test:0.70618556701\n",
      "Trial with: 10 100\n",
      "Train:0.765409934171\n",
      "Test:0.711340206186\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def trial(the_size,the_iter):\n",
    "    global model\n",
    "    print(\"Trial with:\",the_size,the_iter)\n",
    "    model = gensim.models.doc2vec.Doc2Vec(size=the_size, min_count=2, iter=the_iter)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    print('Train:',end='')\n",
    "    tester(train_corpus)\n",
    "    print('Test:',end='')\n",
    "    tester(test_corpus)\n",
    "for i in range(5,105,5):\n",
    "    trial(the_size=10,the_iter=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Train\n",
    "\n",
    "If the BLAS library is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2 minutes, so use BLAS if you value your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_with size=10&iter=100,EEonly')\n",
    "#%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note is that you can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the `model.infer_vector` function. This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_vector([u'데이터', u'알고리즘', u'소프트웨어'])\n",
    "inferred_vector = model.infer_vector(train_corpus[0].words)\n",
    "model.docvecs.most_similar([inferred_vector], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess our new model, we'll first infer new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity. Basically, we're pretending as if the training corpus is some new unseen data and then seeing how they compare with the trained model. The expectation is that we've likely overfit our model (i.e., all of the ranks will be less than 2) and so we should be able to find similar documents very easily. Additionally, we'll keep track of the second ranks for a comparison of less similar documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(corpus):\n",
    "    total_success = 0\n",
    "    total_data = 0\n",
    "    #WARNING : remove divide 4\n",
    "    for doc_id in range(len(corpus)//4):\n",
    "        inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "        sims = model.docvecs.most_similar([inferred_vector], topn=5)\n",
    "        score = corpus[doc_id].tags[0] in [doc_tag for doc_tag, sim in sims]\n",
    "        if score:\n",
    "            total_success += 1\n",
    "        total_data += 1\n",
    "\n",
    "    print(total_success/total_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798922800718\n",
      "0.711340206186\n"
     ]
    }
   ],
   "source": [
    "tester(train_corpus)\n",
    "tester(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('trained_with size=10&iter=195')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most similar to itself and about 5% of the time it is mistakenly most similar to another document. the checking of an inferred-vector against a training-vector is a sort of 'sanity check' as to whether the model is behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document is has a similarity score of ~80% (or higher). However, the similarity score for the second ranked documents should be significantly lower (assuming the documents are in fact different) and the reasoning becomes obvious when we examine the text itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(train_corpus))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same approach above, we'll infer the vector for a randomly chosen test document, and compare the document to our model by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus))\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
